/**
 * 推理Worker
 * 在独立线程中处理模型推理任务，避免阻塞主线程
 */

import worker, { MessageEvents, ErrorEvent } from '@ohos.worker';

/**
 * Worker消息类型
 */
interface WorkerMessage {
  type: 'loadModel' | 'unloadModel' | 'inference' | 'embedding' | 'streamInference';
  id?: string;
  modelPath?: string;
  config?: Record<string, unknown>;
  prompt?: string;
  text?: string;
  data?: Record<string, unknown>;
}

/**
 * Worker响应类型
 */
interface WorkerResponse {
  type: 'modelLoaded' | 'modelUnloaded' | 'inferenceResult' | 'embeddingResult' | 'streamChunk' | 'error';
  id?: string;
  success: boolean;
  data?: unknown;
  error?: string;
}

/**
 * 模型状态接口
 */
interface ModelState {
  isLoaded: boolean;
  modelPath?: string;
  config?: Record<string, unknown>;
  loadTime?: number;
}

interface InferenceResult {
  content: string;
  finishReason: string;
  usage: {
    promptTokens: number;
    completionTokens: number;
    totalTokens: number;
  };
}

/**
 * 推理Worker类
 */
class InferenceWorker {
  private modelState: ModelState = { isLoaded: false };
  private workerPort: any;

  constructor() {
    this.workerPort = worker.workerPort;
    this.setupMessageHandler();
    console.log('[InferenceWorker] Worker初始化完成');
  }

  /**
   * 设置消息处理器
   */
  private setupMessageHandler(): void {
    this.workerPort.onmessage = (event: MessageEvents) => {
      this.handleMessage(event.data as WorkerMessage);
    };

    this.workerPort.onmessageerror = (event: ErrorEvent) => {
      console.error('[InferenceWorker] 消息错误:', event);
      this.sendResponse({
        type: 'error',
        success: false,
        error: '消息处理错误'
      });
    };
  }

  /**
   * 处理消息
   */
  private async handleMessage(message: WorkerMessage): Promise<void> {
    try {
      console.log(`[InferenceWorker] 收到消息: ${message.type}`);

      switch (message.type) {
        case 'loadModel':
          await this.handleLoadModel(message);
          break;
        case 'unloadModel':
          await this.handleUnloadModel(message);
          break;
        case 'inference':
          await this.handleInference(message);
          break;
        case 'embedding':
          await this.handleEmbedding(message);
          break;
        case 'streamInference':
          await this.handleStreamInference(message);
          break;
        default:
          throw new Error(`未知消息类型: ${message.type}`);
      }
    } catch (error) {
      console.error('[InferenceWorker] 处理消息失败:', error);
      this.sendResponse({
        type: 'error',
        id: message.id,
        success: false,
        error: error instanceof Error ? error.message : '未知错误'
      });
    }
  }

  /**
   * 处理加载模型
   */
  private async handleLoadModel(message: WorkerMessage): Promise<void> {
    try {
      if (!message.modelPath) {
        throw new Error('缺少模型路径');
      }

      console.log(`[InferenceWorker] 开始加载模型: ${message.modelPath}`);

      // 如果已有模型加载，先卸载
      if (this.modelState.isLoaded) {
        await this.unloadCurrentModel();
      }

      // 模拟模型加载过程
      await this.simulateModelLoading(message.modelPath, message.config);

      // 更新模型状态
      this.modelState = {
        isLoaded: true,
        modelPath: message.modelPath,
        config: message.config,
        loadTime: Date.now()
      };

      this.sendResponse({
        type: 'modelLoaded',
        id: message.id,
        success: true,
        data: {
          modelPath: message.modelPath,
          loadTime: this.modelState.loadTime
        }
      });

      console.log(`[InferenceWorker] 模型加载完成: ${message.modelPath}`);

    } catch (error) {
      console.error('[InferenceWorker] 加载模型失败:', error);
      this.sendResponse({
        type: 'error',
        id: message.id,
        success: false,
        error: error instanceof Error ? error.message : '加载模型失败'
      });
    }
  }

  /**
   * 处理卸载模型
   */
  private async handleUnloadModel(message: WorkerMessage): Promise<void> {
    try {
      console.log('[InferenceWorker] 开始卸载模型');

      await this.unloadCurrentModel();

      this.sendResponse({
        type: 'modelUnloaded',
        id: message.id,
        success: true
      });

      console.log('[InferenceWorker] 模型卸载完成');

    } catch (error) {
      console.error('[InferenceWorker] 卸载模型失败:', error);
      this.sendResponse({
        type: 'error',
        id: message.id,
        success: false,
        error: error instanceof Error ? error.message : '卸载模型失败'
      });
    }
  }

  /**
   * 处理推理请求
   */
  private async handleInference(message: WorkerMessage): Promise<void> {
    try {
      if (!this.modelState.isLoaded) {
        throw new Error('模型未加载');
      }

      if (!message.prompt) {
        throw new Error('缺少提示文本');
      }

      console.log('[InferenceWorker] 开始推理');

      // 执行推理
      const result = await this.performInference(message.prompt, message.config);

      this.sendResponse({
        type: 'inferenceResult',
        id: message.id,
        success: true,
        data: result
      });

      console.log('[InferenceWorker] 推理完成');

    } catch (error) {
      console.error('[InferenceWorker] 推理失败:', error);
      this.sendResponse({
        type: 'error',
        id: message.id,
        success: false,
        error: error instanceof Error ? error.message : '推理失败'
      });
    }
  }

  /**
   * 处理向量化请求
   */
  private async handleEmbedding(message: WorkerMessage): Promise<void> {
    try {
      if (!this.modelState.isLoaded) {
        throw new Error('模型未加载');
      }

      if (!message.text) {
        throw new Error('缺少文本内容');
      }

      console.log('[InferenceWorker] 开始向量化');

      // 执行向量化
      const embedding = await this.performEmbedding(message.text);

      this.sendResponse({
        type: 'embeddingResult',
        id: message.id,
        success: true,
        data: { embedding }
      });

      console.log('[InferenceWorker] 向量化完成');

    } catch (error) {
      console.error('[InferenceWorker] 向量化失败:', error);
      this.sendResponse({
        type: 'error',
        id: message.id,
        success: false,
        error: error instanceof Error ? error.message : '向量化失败'
      });
    }
  }

  /**
   * 处理流式推理请求
   */
  private async handleStreamInference(message: WorkerMessage): Promise<void> {
    try {
      if (!this.modelState.isLoaded) {
        throw new Error('模型未加载');
      }

      if (!message.prompt) {
        throw new Error('缺少提示文本');
      }

      console.log('[InferenceWorker] 开始流式推理');

      // 执行流式推理
      await this.performStreamInference(message.prompt, message.config, message.id);

      console.log('[InferenceWorker] 流式推理完成');

    } catch (error) {
      console.error('[InferenceWorker] 流式推理失败:', error);
      this.sendResponse({
        type: 'error',
        id: message.id,
        success: false,
        error: error instanceof Error ? error.message : '流式推理失败'
      });
    }
  }

  /**
   * 模拟模型加载
   */
  private async simulateModelLoading(modelPath: string, config: Record<string, unknown>): Promise<void> {
    // 模拟加载时间（根据模型大小调整）
    const loadingTime = 2000 + Math.random() * 3000; // 2-5秒
    
    console.log(`[InferenceWorker] 模拟加载模型，预计耗时: ${Math.round(loadingTime)}ms`);
    
    // 分阶段模拟加载过程
    const stages = [
      '读取模型文件',
      '解析模型结构',
      '初始化推理引擎',
      '加载模型权重',
      '优化计算图',
      '准备推理环境'
    ];

    const stageTime = loadingTime / stages.length;
    
    for (let i = 0; i < stages.length; i++) {
      await new Promise(resolve => setTimeout(resolve, stageTime));
      console.log(`[InferenceWorker] ${stages[i]}... (${i + 1}/${stages.length})`);
    }

    console.log('[InferenceWorker] 模型加载模拟完成');
  }

  /**
   * 卸载当前模型
   */
  private async unloadCurrentModel(): Promise<void> {
    if (this.modelState.isLoaded) {
      // 模拟卸载过程
      await new Promise(resolve => setTimeout(resolve, 500));
      
      this.modelState = { isLoaded: false };
      console.log('[InferenceWorker] 当前模型已卸载');
    }
  }

  /**
   * 执行推理（模拟实现）
   */
  private async performInference(prompt: string, config: Record<string, unknown>): Promise<InferenceResult> {
    // 模拟推理计算时间
    const inferenceTime = 1000 + Math.random() * 2000; // 1-3秒
    await new Promise(resolve => setTimeout(resolve, inferenceTime));

    // 生成模拟响应
    const responses = [
      '我是LingAI助手，很高兴为您服务！有什么我可以帮助您的吗？',
      '这是一个很有趣的问题。让我来为您详细解答...',
      '根据您的描述，我建议您可以尝试以下几种方法：\n1. 首先分析问题的根本原因\n2. 制定详细的解决方案\n3. 逐步实施并监控效果',
      '感谢您的提问。基于我的理解，这个问题可以从多个角度来分析：\n\n技术角度：需要考虑系统架构和实现细节\n业务角度：要关注用户需求和商业价值\n运营角度：需要考虑维护成本和扩展性',
      '我理解您的需求。让我为您提供一些实用的建议和解决方案。\n\n首先，我们需要明确目标和约束条件，然后制定可行的实施计划。'
    ];

    const content = responses[Math.floor(Math.random() * responses.length)];
    
    return {
      content: content,
      finishReason: 'stop',
      usage: {
        promptTokens: this.countTokens(prompt),
        completionTokens: this.countTokens(content),
        totalTokens: this.countTokens(prompt) + this.countTokens(content)
      }
    };
  }

  /**
   * 执行向量化（模拟实现）
   */
  private async performEmbedding(text: string): Promise<number[]> {
    // 模拟向量化计算时间
    const embeddingTime = 200 + Math.random() * 500; // 200-700ms
    await new Promise(resolve => setTimeout(resolve, embeddingTime));

    // 生成模拟向量（384维）
    const embedding: number[] = [];
    const seed = this.hashString(text); // 基于文本内容生成种子，确保相同文本得到相同向量
    
    for (let i = 0; i < 384; i++) {
      // 使用伪随机数生成器，确保结果可重现
      const value = this.seededRandom(seed + i) * 2 - 1; // -1到1之间
      embedding.push(value);
    }
    
    // 归一化向量
    const norm = Math.sqrt(embedding.reduce((sum, val) => sum + val * val, 0));
    return embedding.map(val => val / norm);
  }

  /**
   * 执行流式推理（模拟实现）
   */
  private async performStreamInference(prompt: string, config: Record<string, unknown>, requestId?: string): Promise<void> {
    const fullResponse = '我是LingAI助手，很高兴为您服务！有什么我可以帮助您的吗？让我为您提供详细的解答和建议。';
    const words = fullResponse.split('');
    
    for (let i = 0; i < words.length; i++) {
      // 模拟生成延迟
      await new Promise(resolve => setTimeout(resolve, 50 + Math.random() * 100));
      
      // 发送流式数据块
      this.sendResponse({
        type: 'streamChunk',
        id: requestId,
        success: true,
        data: {
          content: words[i],
          index: i,
          isComplete: false
        }
      });
    }
    
    // 发送完成标记
    this.sendResponse({
      type: 'streamChunk',
      id: requestId,
      success: true,
      data: {
        content: '',
        index: words.length,
        isComplete: true,
        finishReason: 'stop',
        usage: {
          promptTokens: this.countTokens(prompt),
          completionTokens: this.countTokens(fullResponse),
          totalTokens: this.countTokens(prompt) + this.countTokens(fullResponse)
        }
      }
    });
  }

  /**
   * 发送响应
   */
  private sendResponse(response: WorkerResponse): void {
    try {
      this.workerPort.postMessage(response);
    } catch (error) {
      console.error('[InferenceWorker] 发送响应失败:', error);
    }
  }

  /**
   * 计算Token数量（简单实现）
   */
  private countTokens(text: string): number {
    return text.split(/\s+|[.,!?;:]/).filter(token => token.length > 0).length;
  }

  /**
   * 字符串哈希函数
   */
  private hashString(str: string): number {
    let hash = 0;
    for (let i = 0; i < str.length; i++) {
      const char = str.charCodeAt(i);
      hash = ((hash << 5) - hash) + char;
      hash = hash & hash; // 转换为32位整数
    }
    return Math.abs(hash);
  }

  /**
   * 基于种子的伪随机数生成器
   */
  private seededRandom(seed: number): number {
    const x = Math.sin(seed) * 10000;
    return x - Math.floor(x);
  }

  /**
   * 获取Worker状态
   */
  getWorkerStatus(): { modelLoaded: boolean; modelPath?: string; loadTime?: number; uptime: number } {
    return {
      modelLoaded: this.modelState.isLoaded,
      modelPath: this.modelState.modelPath,
      loadTime: this.modelState.loadTime,
      uptime: Date.now() - (this.modelState.loadTime || Date.now())
    };
  }
}

// 创建Worker实例
const inferenceWorker = new InferenceWorker();

// 导出Worker状态查询函数（用于调试）
(globalThis as Record<string, unknown>).getWorkerStatus = () => inferenceWorker.getWorkerStatus();
